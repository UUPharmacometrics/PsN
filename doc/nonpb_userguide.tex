\input{inputs/format_header.tex}
\guidetitle{NONPB and PIND user guides}{2018-01-31}


\begin{document}

\maketitle
\newcommand{\guidetoolname}{nonpb}


\section{Introduction}

The nonpb tool is a resampling-based method for estimating imprecision in nonparametric distribution (npd) estimates obtained in NONMEM. Imprecision in the npd can be estimated by means of two different resampling procedures. \cite{Baverel}.

The simplified method, -nonpb\_version=1, is the default and relies on bootstrap sampling of individual nonparametric probability distributions. 

The full method, -nonpb\_version=2, relies on bootstrap sampling from the raw data and a re-estimation of both the preceding parametric (FOCE) and the nonparametric step.

Nonparametric confidence intervals are computed. In addition of providing information about the precision of nonparametric parameter estimates, the nonpb methods can serve as diagnostic tools for the detection of misspecified parameter distributions.

The P individuals method is availabe as a separate PsN tool, pind.\\
Examples:
\begin{verbatim}
nonpb run12.mod -samples=500

pind run12.mod …
\end{verbatim}

\section{Input and options}
\subsection{Required input nonpb}
A model file is required on the command line as well as the option -samples.
\begin{optionlist}
\optdefault{samples}{N}
The number of samples in the bootstrap. 
\nextopt
\end{optionlist}
\subsection{Optional input nonpb}
\begin{optionlist}
\optdefault{lst\_file}{file.lst}
Default is the same name as the model file but with .mod replaced with .lst. The 
lst-file from where to read initial parameter estimates. 
\nextopt
\optdefault{nonpb\_version}{1 | 2}
Default is 1, the simplified version. Version 2 is the full version. The version of 
the script to run.
\nextopt
\end{optionlist}

\subsection{Required input pind}
A model file is required on the command line.
\subsection{Optional input pind}
\begin{optionlist}
\optdefault{ignore\_individuals\_above}{integer}
Experimental. Only allowed together with -njd, in that case the default value is 100. Option adds line IGNORE=(ID.GT.X) to DATA record, where X is option value. If the IGNORE statement should not be changed even though option -njd is used, set 
-ignore\_individuals\_above=0.
\nextopt
\optdefault{-ind\_param}{eta | theta}
Default eta. The parameter to set in individual ofv model files. Possibilities are eta or theta.
\nextopt
\optdefault{lst\_file}{file.lst}
Default is the name of the input model with a .mod extension replaced with .lst. Name of the lst-file with estimates for the input model. 
\nextopt
\optdefault{njd}{filename}
Default not set. Experimental. Filename for new joint density column.
\nextopt
\end{optionlist}

\subsection{PsN common options}
For a complete list see common\_options.pdf or type psn\_options -h on the command line.

\section{Output}
Intermediate files written during the procedure are found in the intermediate\_files subdirectory of the nonpb run directory. In the result\_files subdirectory the files CI\_results\_ETAk.csv are found, where k=1,...,(number of ETAs). CI\_results\_ETAj.csv contains the mean, median, 25\textsuperscript{th}, 50\textsuperscript{th}, 90\textsuperscript{th} and 95\textsuperscript{th} confidence intervals.

\section{Technical overview of algorithm}
\begin{enumerate}
\item Only if version 2 – full (version 1 simplified starts at step 3) : Create S bootstrapped datasets and estimate the input model with each of the S bootstrap datasets. 
\item For input model (using original data, original model file and original final parameter estimates) and for each bootstrapped model, S+1 in total: Update initial estimates from the final parameter estimates of the bootstrap output run. 
\item If version 2 (full): For S models from step 2 (excluding original model file), compute individual probabilities using procedure described in P-individuals section – this procedure ends up with S files P\_values\_j.csv, j=1,...,S and S table files bs\_model\_j.patab. For the original model file from step 2, modify and run model according to steps 1a-1h of P-individuals section, giving table file original.patab.
	If version 1 (simplified): Compute individual probabilities for single input model using 	procedure described in P-individuals section – this procedure gives the file P\_values.csv and the 	table file original.patab.
\item If version 2 (full): For each P\_values\_j.csv file, bootstrap individual vectors (columns) according to the bootstrap scheme in step 1; e.g. pick up individual vectors corresponding to the individuals contained in the bs\_pr1\_j.dta – name this new file P\_values\_bootstrapped\_j.csv
	If version 1 (simplified): bootstrap individual vectors (columns) of P\_values.csv according to a 	newly generated bootstrap scheme S times. Name the S new files P\_values\_bootstrapped\_j.csv, 	j=1,...,S
\item For each P\_values\_bootstrapped\_j.csv file, sum up rows.
\item Create a new file called bootstrapped\_np\_probabilities\_j.csv which will contain ID column, all ETA values (ETA1-ETAX) from table file from step 3, if version 2 from bs\_model\_j.patab or if version 1 from original.patab S times, and BOTP column containing sum of the rows from step 5 (in total 2+X columns, X being number of ETAs). Note: Sum column represents probabilities.
\item For each bootstrapped\_np\_probabilities\_j.csv, for each ETA\_k in file bootstrapped\_np\_probabilities\_j.csv, transform BOTP column into cumulative probability:
	For each ETA\_k:
\begin{itemize}
	\item Sort the ETA column and BOTP column ascending (according the ETA value) 
	\item Transform BOTP column into cumulative probability column (add them up cumulatively)
	\item Output file(s) (X files for each j) called bootstrap\_CBJD\_ETAk\_j.csv which will contain 3 	columns: ID, ETA\_k, cumulative bootstrapped JD (CBJD))
\end{itemize}
\item For each ETA\_k, create a final file row1\_results\_ETA\_k.csv. From the file bootstrap\_CBJD\_ETAk\_j.csv, copy columns ETA\_k and CBJD and rename them ETAk\_j and CBJD\_j (will refer to these two columns as jth column pair in following text)
\item From a file row1\_results\_ETA\_k.csv, create a new file,\\ bootstrap\_CBJD\_ETAk\_adjusted.csv in which each ETAk\_j vector will be replaced with the ETA\_k vector from the original.patab from step 3 (sorted ascending).\\Each CBJD\_j vector will be adjusted so (CBJD\_j\_adjusted), that for each element of the ETA\_k vector in the Xth column, CBJD\_j value corresponding for the first lower element (first ETAk\_j value lower than ETAk value) of the ETAk\_j column will be chosen. 
\item Compute confidence intervals (3*X files): a) Sort ascending ETAk values from original.patab from step 3 b) For each ETAk create a file called row2\_results\_ETAk.csv which will contain S+1 columns and i rows. First column contains ascending original ETAi values from step a). S columns contain X values of and CBJD\_j\_adjusted found in step 9, each row represents results for each ETAk\_i.
\item For each ETAk: For each ETAi (row) in file row2\_results\_ETAk.csv, read out the mean, median, 25th, 50th, 90th and 95th confidence intervals (based on N values). Store results in CI\_results\_ETAk.csv which will contain 11 columns and I rows
\end{enumerate}

\section{P individuals procedure}

\begin{enumerate}
\item
	\begin{enumerate}
		\item Update initial estimates in the input model from the final parameter estimates and FIX them (THETA/OMEGA and SIGMA)
		\item Skip estimation step by setting MAXEVALS=0
		\item Add \$NONPARAMETRIC UNCONDITIONAL
		\item Add in \$PK if it exists, otherwise in \$PRED: 
	JD = DEN\_ 
	DNj = CDEN\_(j) 
	where j= 1...X (which gives X rows) and where X is the number of ETAs in original model.
		\item Add a nptab table to print ID, JD, ETAX and DNj. Set FIRSTONLY.
		\item Remove \$COVARIANCE
		\item Remove all other existing tables of original model file.
		\item Run the model	(lst file named “np.lst”)
	\end{enumerate}
\item Read the nptab file produced in the previous step and create matrix that contains the Joint Density (JD) in a column (output the JD column of nptab into a separate file called jdtab). The sum of values in jdtab should equal 1 (within rounding error).
\item For each row of the jdtab, denoted “i”:
	\begin{enumerate}
		\item create a copy of the modified model from step 1 and replace each ETAj in the \$PK, \$PRED and \$ERROR records in the model file with the value (number) for that ETAj obtained from the nptab file.
		\item Add DATA=(ID) to \$CONTR.
		\item Add \$SUBROUTINE record if not already present in modelfile.
		\item Add CONTR=iofvcont.f to \$SUBROUTINE, where iofvcont.f is custom CONTR 	routine that prints individual objective function values to file "fort.80"
		\item Fix OMEGAs to zero
		\item Remove \$TABLE and \$NONPARAMETRIC
		\item Run the model with MAXEVAL=0 (lst file named “ofv\_i.lst”)
		\item Extract the last N lines from fort.80 file into a new file called "ofv\_model\_i", where N is the number of individuals.
	\end{enumerate}
\item From the N files called "ofv\_model\_i" create a new file called “ind\_ofv”. This file will have N columns and N rows and should be created using the following principle: starting with i=1, the values of "ofv\_model\_i" should be transposed to generate the first row of the file “ind\_ofv”. Thereafter the same procedure with i=2, 3, …N. 
\item Computation of Likelihood and individual probabilities: For each value (iOFV) in the matrix ind\_ofv, calculate:  iL = exp(-iOFV/2). Then do a scalar multiplication between this matrix (iL) and the matrix with JD values (from jdtab such as 
	[row1 (iL) (N values)*row1 (JD) (single value), row2 (iL)*row2 (JD)….] 
	(output into file “L\_times\_P.csv” which thus will have dimension = N * N)
\item Sum up each column in the resulting L\_times\_P.csv matrix. Output it as sum\_LP.csv file (dimensions: 1(row)*N (columns)
\item Divide each column vector (each value in the column)  of L\_times\_P.csv matrix, with the value from the corresponding column of sum\_LP.csv (single value) and divide it with N. (output it as P\_values.csv file)
\end{enumerate}

\references

\end{document}
