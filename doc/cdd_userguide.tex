\input{inputs/format_header.tex}
\guidetitle{CDD user guide}{2018-09-28}

\usepackage{hyperref}

\begin{document}

\maketitle
\newcommand{\guidetoolname}{cdd}
\tableofcontents
\newpage
\section{Introduction}
The cdd tool (Case Deletion Diagnostics) is run using the command from the command line with a few mandatory arguments. 
The cdd tool is run as a diagnostic after a model is regarded finished or at least mature enough to run validation tool on. 
The cdd algorithm is primarily used to identify influential components of the dataset, usually individuals. The cdd works by identifying groups in the data set and creating one new data set for each member of the group, where that member has been removed. The model is run once with each new data set. The PsN implementation of the cdd can take any column as base for the grouping and all rows with the same value in that column will be considered a group as long as no individual contain multiple values in that column. One should take care that the grouping creates sensible individual records. PsN will renumber the ID column so that two individuals with the same ID will not end up next to each other.

Example:
\begin{verbatim}
cdd run89.mod -case_column=10
\end{verbatim}
This will perform a cdd on the model specified in run89.mod based on the factors in column ten. If for example, 
column ten holds the ids of the seven centers included in the study, this command will create seven copies of the dataset, each with 
individuals included in one specific center deleted. Say that the centers are numbered 1 to 7. Then dataset 1 will have individuals from 
center 1 excluded, dataset 2 individuals from center 2 and so on.

\section{Input and options}

\subsection{Required input}
A NONMEM model file with successful termination is required on the command line. If the model was not run (missing .lst file) or there is no .phi file available it will be run to produce these files.
\begin{optionlist}
\optdefault{case\_column}{name|number}
Default is ID. Through this flag you set the column on which the case-deletion is done. You can either use the name of the column as specified in the \mbox{\$INPUT} record in the model file or you can use the column number in the \mbox{\$INPUT} record. Numbering starts with 1. 
\end{optionlist}

\subsection {Optional input}
\begin{optionlist}
\optdefault{bins}{N}
Default value = The number of unique values in the case column. Sets the number of databins, or cdd datasets, to use. If the number of unique values, or factors, in the case column is higher than N then one or more factors will be deleted in each cdd dataset. Specifying N as higher than the number of factors will have no effect. N is then reset to the number of factors. 
\nextopt
\optname{etas}
Use the .phi file of the original model as initial estimates for the etas.
\nextopt
\optname{ignore}
Instead of generating new datasets use IGNORE statements in \$DATA. Default is off.
\nextopt
\optname{last\_est\_complete}
is optional and only applies with NONMEM 7 and cdd option -xv. 
\optdefault{outside\_n\_sd\_check}{X}
Default is 2. Mark the runs with Cook score - Covariance ratio outside X standard deviations of the principal component analysis (PCA). 
\nextopt
\optdefault{selection\_method}{random | consecutive}
Default consecutive. Specifies whether the factors selected for exclusion should be drawn randomly or consecutively from the datafile. 
\nextopt
\optname{update\_inits}
By default, the initial estimates of the cdd models will be set to the final estimates of the input model if final
estimates are available. If the user sets -no-update\_inits, the initial estimates of the cdd models will be
the same as the initial estimates set in the input model, even if final estimates from the input model are available.
\nextopt
\optname{xv}
Default false. Run the cross-validation step (-xv) or not (-no-xv). 
\nextopt
\end{optionlist}
			
\subsection{PsN common options}
For a complete list see common\_options.pdf or type psn\_options -h on the command line.
		
\subsection{cdd R plots}
\newcommand{\rplotsconditions}{
If option -rplots is set $>=1$, a plot with Covariance ratios
vs Cook scores for each case, e.g. ID, will be generated. 
The default cdd R plots template 
requires no special R libraries.
If no pdf is generated,
see the .Rout file in the main run directory for error messages.}
\input{inputs/rplots_section_body.tex}

\section{Output}

Below, $P_{orig}$ denotes the vector of estimated parameters (theta, omega and sigma)
on the original dataset, $p_{orig,j}$ is the $j$th element of $P_{orig}$,
$P_{i}$ denotes the vector of estimated parameters (theta, omega and sigma)
on the dataset with case $i$ excluded, $p_{i,j}$ is the $j$th element of $P_{i}$,
$cov(Y)$ is the covariance matrix of $Y$,
$se(y)$ is the standard error of $y$,
$N$ is the number of cases in the dataset,
and
$det(Y)$ is the determinant of $Y$.

\[
p_{(\cdot),j}=\frac{1}{N}\sum_{i=1}^{N}p_{i,j} 
\]
is the mean of $p_{i,j}$ over all case deleted datasets, 
\[
cov^{jackknife}_{j,k} = \frac{N-1}{N}\sum_{i=1}^{N}\left(p_{i,j}-p_{(\cdot),j}\right)\left(p_{i,k}-p_{(\cdot),k}\right)
\]
is the jackknife estimate of the covariance between $p_{orig,j}$ and $p_{orig,k}$, and this is used
to compute the full matrix
$cov^{jackknife}(P_{orig})$. If any case deleted dataset fails to give parameter estimates $P_{i}$ then
jackknife estimates of the covariances will not be computed.

\subsubsection*{Raw results file}
The standard PsN raw results file has a set of columns with cdd-specific results appended
for each case deleted dataset $i$:
\begin{description}
\item[cook.scores] $\sqrt{\left(P_{i}-P_{orig}\right)^Tcov(P_{orig})^{-1}\left(P_{i}-P_{orig}\right)}$\\
Only computed if $cov(P_{orig})$ is available.
\item[jackknife.cook.scores] $\sqrt{\left(P_{i}-P_{orig}\right)^Tcov^{jackknife}(P_{orig})^{-1}\left(P_{i}-P_{orig}\right)}$
\item[cov.ratios] \[\sqrt{\frac{det(cov(P_{i}))}{det(cov(P_{orig}))}}\]
Only computed if $cov(P_{orig})$ is available.
Set to 0 if $cov(P_{i})$ not available or numerically singular.
\item[outside.n.sd] 1 or 0, only relevant if both Cook scores and Cov ratios available.
1 if Cook score - Covariance ratio is outside 'outside\_n\_sd\_check' standard deviations of the PCA. 
\item[cdd.delta.ofv] This value is computed for each case deleted dataset i and
contains the difference $OFV_{cdd-i,orig} - OFV_{cdd-i,est}$,
where $OFV_{cdd-i,orig}$ is the sum of individual OFV (iOFV) for the individuals in case deleted dataset $i$ (cdd-i) 
from an estimation where all subjects are included and
$OFV_{cdd-i,est}$ is the OFV from an estimation where only the subjects in cdd-i are included. 
This value is always positive.
\item[cook.par.<param j>] Individual Cook score for parameter <param j>,
\[\frac{abs\left(p_{i,j}-p_{orig,j}\right)}{se(p_{orig,j})} \]
\item[jack.cook.par.<param j>] Individual jackknife approximated Cook score for parameter <param j>,
\[\frac{abs\left(p_{i,j}-p_{orig,j}\right)}{\sqrt{cov^{jackknife}_{j,j}}}\]
\end{description}

If the cross-validation was requested with option -xv, there are $N$ extra rows in the raw results file.
Each line is the result from evaluation of the model on the remainder data using parameter values from
estimation of the corresponding case-deleted dataset.

\subsubsection*{jackknife.cov}
The file jackknife.cov contains $cov^{jackknife}(P_{orig})$. It is comma separated and has the same number of columns
as the number of estimated (not FIXED) parameters. The column order is theta, omega, sigma, and the
column/row labels are the parameter labels from the model file, if any.

\subsubsection*{cdd\_results.csv}
The Diagnostics section of cdd\_results.csv contains the same computed cook.scores, jackknife.cook.scores, cov.ratios and
outside.n.sd as the raw results file.

The section relative.changes.percent lists, for each case deleted dataset, the relative change in percent
for ofv, estimates and standard errors.

The section Jackknife.bias.estimate has two rows. The jackknife bias of parameter $j$ is \[bias=\left(N-1\right)\left(p_{(\cdot),j}-p_{orig,j}\right) \]
and the relative bias in percent is \[100\cdot\frac{bias}{p_{orig,j}}\].

\section{Known bugs/issues}

If NONMEM 6 is used with cdd and the S matrix is algorithmically singular (message in .lst-file, checked also by the sumo tool) the Cook scores cannot be trusted. The cdd does not check for this error. 

\end{document}
